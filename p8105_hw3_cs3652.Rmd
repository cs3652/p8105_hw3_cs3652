---
title: "P8105_hw3_cs3652"
author: "Chirag Shah"
date: '2018-10-15'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
fig.width = 6,
fig.asp = .6,
out.width = "90%"
)
```

```{r}
library(tidyverse)
library(httr)
library(jsonlite)
library(patchwork)
library(hexbin)
```

##Problem 1

```{r}
library(p8105.datasets)
data("brfss_smart2010")
##reading in data

brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  select(location_abbr = locationabbr, location_desc = locationdesc, year, topic, response, data_value) %>% 
  ##tidying data by cleaning names and renaming
  filter(topic == "Overall Health") %>% 
  ##focusing on Overall Health
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>% 
  mutate(response = factor(response, levels = ordered(c("Excellent", "Very good", "Good", "Fair", "Poor"))))
  ##ordering response variable
```

```{r}
brfss_data %>% 
  select(year, location_abbr, location_desc) %>% 
  filter(year == "2002") %>% 
  ##filtering data for 2002
  distinct(location_desc, location_abbr) %>% 
  group_by(location_abbr) %>% 
  summarize(locations_7 = n()) %>% 
  ##getting number of locations 
  filter(locations_7 == "7")
```

In 2002, CT, FL, and NC were observed at 7 locations. 

```{r}
brfss_data %>% 
  select(year, location_abbr, location_desc) %>% 
  filter(year > 2001, year < 2011) %>%
  ##selecting years from 2002 to 2010
  group_by(location_abbr, year) %>% 
  distinct(location_desc, location_abbr) %>% 
  summarize(locations_per_state = n()) %>%
  ##creating variable for locations per state that are distinct
  ggplot(aes(x = year, y = locations_per_state, color = location_abbr)) +
  geom_line() +
  ##creating speghetti plot
  labs(
    title = "Number of Locations in Each State From 2002-2010",
    x = "year",
    y = "number of locations",
    caption = "data from the BRFSS dataset"
  ) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE
  ) + 
  ggthemes::theme_economist() +
  theme(legend.position = "bottom", legend.text = element_text(size = 6), legend.key.size = unit(2, "point"))
##makeing the graph easier to read and using formats to make it look good
```

```{r}
brfss_data %>%
  spread(key = response, value = data_value) %>%
  ##separating the response variable 
  select(year, Excellent, location_abbr) %>% 
  filter(!is.na(Excellent), location_abbr == "NY", (year == "2002" | year == "2006" | year == "2010")) %>%
  ##choosing exchellent, the years, and the state desired
  group_by(year) %>% 
  summarize(mean_proportion_excellent = mean(Excellent), sd_proportion_excellent = sd(Excellent)) %>% 
  ##getting the mean and standard deviation of the proportion of Excellent
  knitr::kable()
  ##creating good looking table
```

```{r}
brfss_data %>% 
  select(year, location_abbr, response, data_value) %>% 
  group_by(year, location_abbr, response) %>% 
  summarize(avg_response = mean(data_value)) %>% 
  ##getting average proportion for each response category
  ggplot(aes(x = year, y = avg_response, color = location_abbr)) + 
  geom_point() +
  ##creating plot
  labs(
    title = "Average Proportion of Each Response Category in Each State Over Time",
    x = "year",
    y = "average proportion of response",
    caption = "data from BRFSS_2010"
  ) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE
  ) + 
  ggthemes::theme_economist() + 
  theme(legend.position = "bottom", legend.text = element_text(size = 6), legend.key.size = unit(2, "point")) +
  ##makeing the graph easier to read
  facet_grid(~ response) 
  ##panelling by the response variable 
```

##Problem 2

```{r}
library(p8105.datasets)
data("instacart")
##reading in data

instacart_data = instacart %>% 
  janitor::clean_names()
  ##tidying variable names
```


To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. Then, do or answer the following (commenting on the results of each):
`r nrow(instacart_data)` `r ncol(instacart_data)` observations: `r count(instacart_data)` distict users `r instacart_data %>% distinct(user_id) %>% count()` 
key variables: `user_id` `r instacart_data %>% select(user_id) %>% head(1)`
`product_name` `r instacart_data %>% select(product_name) %>% head(1)`
`department` `r instacart_data %>% select(department) %>% head(1)`

```{r}
instacart_data %>% 
  select(aisle) %>% 
  count()
  ##finding the number of aisles

instacart_data %>% 
  group_by(aisle) %>% 
  summarize(amount_ordered = n()) %>%
  ##creating amount ordered variable
  arrange(desc(amount_ordered)) %>% 
  ##ordering from most to least
  select(aisle) %>% 
  head(1)
  ##choosing the top observation
```

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
```{r}
instacart_data %>% 
  group_by(aisle) %>% 
  ##grouping by aisle
  summarize(amount_ordered = n()) %>% 
  ##computing the amount ordered in each aisle after groupby step
  ggplot(aes(x = aisle, y = amount_ordered)) + 
  geom_point() +
  ##creating plot
  labs(
     title = "Items ordered in each aisle",
     x = "aisle",
     y = "number of items ordered"
    ) +
    theme_bw() +
    theme(legend.position = "bottom")
  ##making graph easier to read
```

    Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
    
```{r}
instacart_data %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  ##filtering aisles that we want
  group_by(aisle, product_name) %>% 
  ##grouping by aisle product name
  summarize(number_ordered = n()) %>% 
  arrange(desc(number_ordered)) %>% 
  ##going from most to least for number of a specific product ordered
  group_by(aisle) %>% 
  top_n(n = 1) %>% 
  ##taking top observation after grouping by aisle
  rename(most_popular_item = product_name) %>% 
  knitr::kable()
  ##creating table that is easy to read
```

    Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart_data %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  ##filtering products that we want 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(order_dow, product_name) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  ##computing the mean hour of the day that the item was ordered
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()
  ##making readable table
```

##Problem 3 

```{r}
library(p8105.datasets)
data(ny_noaa)
##reading in the data
```

`r nrow(ny_noaa)`
`r ncol(ny_noaa)` 
`r count(ny_noaa)` observations
`r ny_noaa %>% distinct(id) %>% count()` distinct weather stations
key variables: id `r ny_noaa %>% select(id) %>% head(1)` 
date of the weather observation `r ny_noaa %>% select(date) %>% head(1)`
precipitation that occurred 
tmax and tmin variables give maximum and minimum temperatures
missing data values in this data set for prcp, tmin, and tmax `r ny_noaa %>% filter(is.na(tmax) | is.na(tmin) | is.na(prcp) | is.na(snow) | is.na(snwd)) %>% nrow()` 

```{r}
ny_noaa_data = ny_noaa %>%
  janitor::clean_names() %>% 
  ##cleaning the variable names
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  ##separating the date into 3 new variables
  mutate(prcp = prcp / 10, tmax = as.integer(tmax) / 10, tmin = as.integer(tmin) / 10)
  ##change to appropriate units since prcp was in tenths of mm and temperatures were in tenths of degrees celcius 
ny_noaa_data %>% 
  group_by(snow) %>% 
  summarize(freq_observed = n()) %>% 
  ##creating frequency observed variable
  arrange(desc(freq_observed)) %>% 
  ##ordering from most observed frequency to least
  head(1)
  ##getting top observation after arragement 
```

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ny_noaa_data %>% 
  filter(tmax != "NA", (month == "01" | month == "07")) %>% 
  group_by(id, year, month) %>% 
  summarize(avg_tmax = mean(tmax)) %>%
  ##computing the average max temperature
  ggplot(aes(x = year, y = avg_tmax, color = id)) +
    geom_point() +
  ##creating plot of max temperature for each individual id
  scale_x_discrete(breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
  ##showing select years on the x axis to be more readable 
  facet_grid(~month) +  
  ##pannelling by the two months we selected earlier
  labs(
      x = "year",
      y = "average temp (°C)",
      title = "Average Maximum Temperatures for January and July in New York Weather Stations Over Time",
      caption = "Data from NOAA"
    ) +
    viridis::scale_color_viridis(
      discrete = TRUE,
      name = "ID"
    ) +
    ggthemes::theme_economist() +
    theme(legend.position = "bottom", legend.text = element_text(size = 6), legend.key.size = unit(2, "point")) +
    guides(color = guide_legend(nrow = 20))
  ##making the graph easier to read 
```

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
t_max_min_plot = ny_noaa_data %>% 
  filter(tmax != "NA" & tmin != "NA") %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
  ##creating a graph of tmax vs tmin
    geom_hex() + 
  ##choosing appropriate graph type to depict information
  labs(
      x = "max temperature (°C)",
      y = "min temperature (°C)",
      title = "Maximum and minimum temperatures for full dataset",
      caption = "Data from NOAA"
    ) + 
  ggthemes::theme_economist() +
  theme(legend.position = "bottom", legend.text = element_text(size = 6), legend.key.size = unit(2, "point"))
##making the graph easier to read

snowfall_plot = ny_noaa_data %>% 
  filter(snow != "NA" & snow > 0 & snow < 100) %>% 
  ##choosing snowfall creater than 0 and less than 100
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
  ##using boxplot to show distribution of snowfall
    labs(
      x = "year",
      y = "snowfall amount (mm)",
      title = "distribution of snowfall by year",
      caption = "Data from NOAA"
    ) + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, size = 5))
  ##making the graph easy to read

t_max_min_plot / snowfall_plot
```


